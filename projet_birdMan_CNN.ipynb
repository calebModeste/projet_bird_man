{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. INITIALISATION - Imports et configuration du dataset ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration du style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Détecter l'environnement\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALISATION DU PROJET BIRD CLASSIFICATION - CNN CLASSIQUE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEnvironnement : {'Google Colab' if IN_COLAB else 'Python Local'}\")\n",
    "\n",
    "# Variables globales\n",
    "DRIVE_FOLDER_ID = \"1kHTcb7OktpYB9vUaZPLQ3ywXFYMUdQsP\"\n",
    "LOCAL_DATA_PATH = Path(\"./data\")\n",
    "TRAIN_PATH = LOCAL_DATA_PATH / \"train_bird\"\n",
    "VALID_PATH = LOCAL_DATA_PATH / \"valid_bird\"\n",
    "\n",
    "# Initialiser dataset_root\n",
    "dataset_root = None\n",
    "drive_loader = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print(\"\\n[OK] Mode Google Colab detecte\")\n",
    "    print(\"  Montage de Google Drive...\")\n",
    "    \n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        drive_base_path = Path('/content/drive/My Drive')\n",
    "        \n",
    "        # Chercher le dataset\n",
    "        for item in drive_base_path.iterdir():\n",
    "            if item.is_dir() and (item / 'train_bird').exists():\n",
    "                dataset_root = item\n",
    "                print(f\"  [OK] Dataset trouve dans : {item.name}\")\n",
    "                break\n",
    "        \n",
    "        if not dataset_root:\n",
    "            print(\"  [ATTENTION] Dataset non trouve dans My Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ATTENTION] Erreur : {e}\")\n",
    "else:\n",
    "    print(\"\\n[OK] Mode Python Local detecte\")\n",
    "    \n",
    "    # Vérifier les données locales\n",
    "    if TRAIN_PATH.exists() and VALID_PATH.exists():\n",
    "        print(f\"  [OK] Donnees locales trouvees : {LOCAL_DATA_PATH}\")\n",
    "        dataset_root = LOCAL_DATA_PATH\n",
    "    else:\n",
    "        print(f\"  [ATTENTION] Donnees locales non trouvees\")\n",
    "        print(f\"    Chemin attendu : {LOCAL_DATA_PATH}\")\n",
    "        print(f\"    train_bird existe : {TRAIN_PATH.exists()}\")\n",
    "        print(f\"    valid_bird existe : {VALID_PATH.exists()}\")\n",
    "\n",
    "print(\"\\n[OK] Initialisation terminee !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d326e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ANALYSE DU DATASET - Creer un DataFrame avec les informations ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DU DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Limite d'images par classe    \n",
    "MAX_IMAGES_PER_CLASS = 50\n",
    "#MAX_IMAGES_PER_CLASS = 500  # <- LIMITE A 500 IMAGES PAR CLASSE\n",
    "\n",
    "if dataset_root is None:\n",
    "    print(\"\\n[ATTENTION] Dataset non accessible\")\n",
    "    print(\"  Executez la cellule 1 d'abord et assurez-vous que le dataset est disponible\")\n",
    "else:\n",
    "    try:\n",
    "        # Chemins des données\n",
    "        train_dir = Path(dataset_root) / 'train_bird'\n",
    "        valid_dir = Path(dataset_root) / 'valid_bird'\n",
    "        \n",
    "        # Créer les listes de données\n",
    "        data = []\n",
    "        \n",
    "        # Traiter les données d'entraînement\n",
    "        print(\"\\n[OK] Analyse des donnees d'entrainement...\")\n",
    "        if train_dir.exists():\n",
    "            for class_path in sorted(train_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter à MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Entrainement',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        # Traiter les données de validation\n",
    "        print(\"[OK] Analyse des donnees de validation...\")\n",
    "        if valid_dir.exists():\n",
    "            for class_path in sorted(valid_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter à MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Validation',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        if data:\n",
    "            # Créer le DataFrame\n",
    "            df_dataset = pd.DataFrame(data)\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"RESUME DU DATASET\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            n_classes = df_dataset['Classe'].nunique()\n",
    "            total_images = df_dataset[\"Nombre d'images\"].sum()\n",
    "            \n",
    "            print(f\"\\nStatistiques globales :\")\n",
    "            print(f\"   Nombre total de classes : {n_classes}\")\n",
    "            print(f\"   Nombre total d'images : {total_images:,}\")\n",
    "            print(f\"   Limite par classe : {MAX_IMAGES_PER_CLASS} images\")\n",
    "            \n",
    "            print(f\"\\nRepartition par ensemble :\")\n",
    "            stats = df_dataset.groupby('Ensemble').agg({\n",
    "                'Classe': 'nunique',\n",
    "                \"Nombre d'images\": ['sum', 'mean', 'min', 'max']\n",
    "            })\n",
    "            stats.columns = ['Nombre de classes', 'Total images', 'Moy/classe', 'Min', 'Max']\n",
    "            print(stats.to_string())\n",
    "            \n",
    "            print(f\"\\nTop 5 classes par nombre d'images :\")\n",
    "            top_classes = df_dataset.nlargest(5, \"Nombre d'images\")[['Classe', 'Ensemble', \"Nombre d'images\"]]\n",
    "            print(top_classes.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n[OK] DataFrame cree avec succes !\")\n",
    "            print(f\"   Forme : {df_dataset.shape}\")\n",
    "        else:\n",
    "            print(\"[ATTENTION] Aucune image trouvee dans le dataset\")\n",
    "            df_dataset = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERREUR] Erreur lors de l'analyse : {e}\")\n",
    "        df_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf53b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. PREPARATION DES DONNEES - Preprocessing et augmentation OPTIMISES ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARATION DES DONNEES POUR LE DEEP LEARNING (OPTIMISE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Installation de TensorFlow\n",
    "print(\"\\n[OK] Installation de TensorFlow...\")\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\", \"-q\"])\n",
    "    print(\"  [OK] TensorFlow installe\")\n",
    "except:\n",
    "    print(\"  [ATTENTION] TensorFlow deja installe\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n[OK] Configuration des parametres OPTIMISES...\")\n",
    "\n",
    "# Paramètres optimisés\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Réduit pour meilleure généralisation\n",
    "EPOCHS = 30  # Plus d'epochs avec early stopping\n",
    "LEARNING_RATE = 0.0005  # Learning rate plus bas pour convergence stable\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(f\"  Taille des images : {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch size : {BATCH_SIZE} (optimise)\")\n",
    "print(f\"  Nombre d'epochs : {EPOCHS}\")\n",
    "print(f\"  Learning rate : {LEARNING_RATE} (optimise)\")\n",
    "\n",
    "# Générateurs d'images avec augmentation AVANCÉE\n",
    "print(\"\\n[OK] Creation des data generators avec augmentation avancee...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    # Augmentation géométrique\n",
    "    rotation_range=30,  # Rotation plus large\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,  # Zoom plus agressif\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,  # Les oiseaux ne sont pas à l'envers\n",
    "    fill_mode='reflect',  # Meilleur que 'nearest'\n",
    "    # Augmentation colorimétrique\n",
    "    brightness_range=[0.8, 1.2],  # Variation de luminosité\n",
    "    channel_shift_range=30,  # Variation de couleur\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Validation sans augmentation mais avec normalisation\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Charger les données d'entraînement\n",
    "train_dir = Path(dataset_root) / 'train_bird' if dataset_root else TRAIN_PATH\n",
    "\n",
    "try:\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        interpolation='bicubic'  # Meilleure qualité de redimensionnement\n",
    "    )\n",
    "    \n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        interpolation='bicubic'\n",
    "    )\n",
    "    \n",
    "    # Charger les données de test (validation du dataset)\n",
    "    valid_dir = Path(dataset_root) / 'valid_bird' if dataset_root else VALID_PATH\n",
    "    \n",
    "    test_generator = val_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "        interpolation='bicubic'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[OK] Data generators crees avec succes !\")\n",
    "    print(f\"  Train generator : {len(train_generator)} batches\")\n",
    "    print(f\"  Validation generator : {len(val_generator)} batches\")\n",
    "    print(f\"  Test generator : {len(test_generator)} batches\")\n",
    "    print(f\"  Nombre de classes : {train_generator.num_classes}\")\n",
    "    \n",
    "    # Sauvegarder les noms de classes\n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nAugmentations appliquees :\")\n",
    "    print(f\"  Rotation : +/-30deg\")\n",
    "    print(f\"  Decalage H/V : +/-25%\")\n",
    "    print(f\"  Zoom : 70-130%\")\n",
    "    print(f\"  Luminosite : 80-120%\")\n",
    "    print(f\"  Channel shift : +/-30\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERREUR] Erreur lors de la creation des generators : {e}\")\n",
    "    train_generator = None\n",
    "    val_generator = None\n",
    "    test_generator = None\n",
    "    class_names = None\n",
    "    num_classes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. CREATION DU MODELE - CNN OPTIMISE pour classification d'images ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATION DU MODELE CNN OPTIMISE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None:\n",
    "    print(\"\\n[ERREUR] Les data generators ne sont pas disponibles\")\n",
    "    print(\"   Executez la cellule 3 d'abord\")\n",
    "else:\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    \n",
    "    # Créer un modèle CNN optimisé avec régularisation L2\n",
    "    print(\"\\n[OK] Construction du modele CNN optimise...\")\n",
    "    \n",
    "    # Facteur de régularisation L2\n",
    "    L2_REG = 0.001\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # ========== BLOC 1 - Extraction de features bas niveau ==========\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(L2_REG),\n",
    "                      input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.1),  # Dropout spatial plus efficace\n",
    "        \n",
    "        # ========== BLOC 2 - Features intermédiaires ==========\n",
    "        layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.15),\n",
    "        \n",
    "        # ========== BLOC 3 - Features complexes ==========\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.2),\n",
    "        \n",
    "        # ========== BLOC 4 - Features haut niveau ==========\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.25),\n",
    "        \n",
    "        # ========== BLOC 5 - Features très haut niveau ==========\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.GlobalAveragePooling2D(),  # Plus efficace que Flatten\n",
    "        \n",
    "        # ========== CLASSIFICATION ==========\n",
    "        layers.Dense(512, kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(256, kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Optimiseur avec weight decay et momentum\n",
    "    print(\"\\n[OK] Compilation du modele avec optimiseur avance...\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    initial_lr = LEARNING_RATE\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=initial_lr,\n",
    "        weight_decay=0.0001,  # Régularisation additionnelle\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy')]\n",
    "    )\n",
    "    \n",
    "    # Afficher le résumé du modèle\n",
    "    print(\"\\nArchitecture du modele OPTIMISE :\")\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    \n",
    "    print(f\"   Parametres totaux : {total_params:,}\")\n",
    "    print(f\"   Parametres entrainables : {trainable_params:,}\")\n",
    "    print(f\"   Blocs convolutifs : 5\")\n",
    "    print(f\"   GlobalAveragePooling : [OK] (reduit l'overfitting)\")\n",
    "    print(f\"   Regularisation L2 : {L2_REG}\")\n",
    "    print(f\"   SpatialDropout2D : [OK] (plus efficace)\")\n",
    "    \n",
    "    # Callbacks optimisés\n",
    "    print(\"\\n[OK] Configuration des callbacks avances...\")\n",
    "    \n",
    "    # Learning rate scheduler avec warmup\n",
    "    def lr_schedule(epoch, lr):\n",
    "        if epoch < 3:\n",
    "            return lr  # Warmup\n",
    "        elif epoch < 15:\n",
    "            return lr * 0.95  # Décroissance douce\n",
    "        else:\n",
    "            return lr * 0.9  # Décroissance plus rapide\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=8,  # Plus de patience\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_cnn_optimized.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n[OK] Modele CNN OPTIMISE pret pour l'entrainement !\")\n",
    "    print(f\"\\nAmeliorations appliquees :\")\n",
    "    print(f\"  [OK] Architecture VGG-like avec 5 blocs\")\n",
    "    print(f\"  [OK] GlobalAveragePooling (meilleur que Flatten)\")\n",
    "    print(f\"  [OK] SpatialDropout2D (regularisation spatiale)\")\n",
    "    print(f\"  [OK] Regularisation L2 sur toutes les couches\")\n",
    "    print(f\"  [OK] AdamW optimizer avec weight decay\")\n",
    "    print(f\"  [OK] Learning rate scheduler\")\n",
    "    print(f\"  [OK] Top-3 accuracy tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. ENTRAINEMENT - Training du modele CNN OPTIMISE ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRAINEMENT DU MODELE CNN OPTIMISE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None or model is None:\n",
    "    print(\"\\n[ERREUR] Erreur : Les donnees ou le modele ne sont pas disponibles\")\n",
    "    print(\"   Executez les cellules 3 et 4 d'abord\")\n",
    "    history = None\n",
    "else:\n",
    "    try:\n",
    "        import time\n",
    "        \n",
    "        print(f\"\\n[OK] Demarrage de l'entrainement...\")\n",
    "        print(f\"  Epochs : {EPOCHS}\")\n",
    "        print(f\"  Batch size : {BATCH_SIZE}\")\n",
    "        print(f\"  Etapes par epoch : {len(train_generator)}\")\n",
    "        print(f\"  Learning rate initial : {LEARNING_RATE}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entraîner le modèle (sans limitation de steps pour meilleure précision)\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Statistiques finales\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        best_val_top3 = max(history.history['val_top3_accuracy'])\n",
    "        final_lr = history.history.get('lr', [LEARNING_RATE])[-1] if 'lr' in history.history else LEARNING_RATE\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"RESUME DE L'ENTRAINEMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Temps total : {total_time/60:.1f} min\")\n",
    "        print(f\"  Meilleure precision validation : {best_val_acc*100:.2f}%\")\n",
    "        print(f\"  Meilleure Top-3 accuracy : {best_val_top3*100:.2f}%\")\n",
    "        print(f\"  Learning rate final : {final_lr:.2e}\")\n",
    "        print(f\"  Modele sauvegarde : best_model_cnn_optimized.h5\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERREUR] Erreur lors de l'entrainement : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. EVALUATION - Resultats et visualisation ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION DU MODELE CNN OPTIMISE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if history is None or model is None:\n",
    "    print(\"\\n[ERREUR] Erreur : L'entrainement n'a pas eu lieu\")\n",
    "    print(\"   Executez la cellule 5 d'abord\")\n",
    "else:\n",
    "    try:\n",
    "        # Évaluer sur l'ensemble de test\n",
    "        print(\"\\n[OK] Evaluation sur l'ensemble de test...\")\n",
    "        results = model.evaluate(test_generator, verbose=0)\n",
    "        test_loss = results[0]\n",
    "        test_accuracy = results[1]\n",
    "        test_top3 = results[2] if len(results) > 2 else None\n",
    "        \n",
    "        print(f\"\\nResultats sur le test set :\")\n",
    "        print(f\"   Perte test : {test_loss:.4f}\")\n",
    "        print(f\"   Precision test : {test_accuracy*100:.2f}%\")\n",
    "        if test_top3:\n",
    "            print(f\"   Top-3 accuracy : {test_top3*100:.2f}%\")\n",
    "        \n",
    "        # Visualiser l'historique d'entraînement\n",
    "        print(\"\\n[OK] Creation des graphiques...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "        \n",
    "        # Graphique de la perte\n",
    "        axes[0, 0].plot(epochs_range, history.history['loss'], 'b-', label='Perte entrainement', linewidth=2)\n",
    "        axes[0, 0].plot(epochs_range, history.history['val_loss'], 'r-', label='Perte validation', linewidth=2)\n",
    "        axes[0, 0].set_title('Perte au cours de l\\'entrainement', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Perte')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Graphique de la précision\n",
    "        axes[0, 1].plot(epochs_range, history.history['accuracy'], 'b-', label='Precision entrainement', linewidth=2)\n",
    "        axes[0, 1].plot(epochs_range, history.history['val_accuracy'], 'r-', label='Precision validation', linewidth=2)\n",
    "        axes[0, 1].axhline(y=test_accuracy, color='g', linestyle='--', label=f'Test: {test_accuracy*100:.1f}%')\n",
    "        axes[0, 1].set_title('Precision au cours de l\\'entrainement', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Precision')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top-3 accuracy\n",
    "        if 'top3_accuracy' in history.history:\n",
    "            axes[1, 0].plot(epochs_range, history.history['top3_accuracy'], 'b-', label='Top-3 entrainement', linewidth=2)\n",
    "            axes[1, 0].plot(epochs_range, history.history['val_top3_accuracy'], 'r-', label='Top-3 validation', linewidth=2)\n",
    "            if test_top3:\n",
    "                axes[1, 0].axhline(y=test_top3, color='g', linestyle='--', label=f'Test: {test_top3*100:.1f}%')\n",
    "            axes[1, 0].set_title('Top-3 Accuracy', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate (si disponible)\n",
    "        if 'lr' in history.history:\n",
    "            axes[1, 1].plot(epochs_range, history.history['lr'], 'g-', linewidth=2)\n",
    "            axes[1, 1].set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Learning Rate')\n",
    "            axes[1, 1].set_yscale('log')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            # Gap entre train et val accuracy\n",
    "            train_acc = np.array(history.history['accuracy'])\n",
    "            val_acc = np.array(history.history['val_accuracy'])\n",
    "            gap = train_acc - val_acc\n",
    "            axes[1, 1].fill_between(epochs_range, gap, alpha=0.3, color='red')\n",
    "            axes[1, 1].plot(epochs_range, gap, 'r-', linewidth=2)\n",
    "            axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            axes[1, 1].set_title('Overfitting Gap (Train - Val)', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Gap')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history_cnn_optimized.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n[OK] Graphiques affiches et sauvegardes !\")\n",
    "        \n",
    "        # Prédictions sur quelques images de test\n",
    "        print(\"\\n[OK] Test de prediction sur des images...\")\n",
    "        \n",
    "        # Récupérer quelques images du test\n",
    "        test_generator.reset()\n",
    "        test_images, test_labels = next(test_generator)\n",
    "        \n",
    "        # Faire des prédictions\n",
    "        predictions = model.predict(test_images[:9], verbose=0)\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(test_labels[:9], axis=1)\n",
    "        \n",
    "        # Afficher les résultats avec Top-3\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx in range(9):\n",
    "            img = (test_images[idx] * 255).astype(np.uint8)\n",
    "            true_label = class_names[true_classes[idx]]\n",
    "            \n",
    "            # Top-3 predictions\n",
    "            top3_idx = np.argsort(predictions[idx])[::-1][:3]\n",
    "            top3_labels = [class_names[i] for i in top3_idx]\n",
    "            top3_probs = [predictions[idx][i] * 100 for i in top3_idx]\n",
    "            \n",
    "            axes[idx].imshow(img)\n",
    "            \n",
    "            # Couleur selon si correct\n",
    "            is_correct = true_classes[idx] == pred_classes[idx]\n",
    "            in_top3 = true_classes[idx] in top3_idx\n",
    "            \n",
    "            if is_correct:\n",
    "                color = 'green'\n",
    "                status = '[OK]'\n",
    "            elif in_top3:\n",
    "                color = 'orange'\n",
    "                status = '[~]'\n",
    "            else:\n",
    "                color = 'red'\n",
    "                status = '[X]'\n",
    "            \n",
    "            title = f'{status} Vrai: {true_label}\\n'\n",
    "            title += f'1. {top3_labels[0]} ({top3_probs[0]:.1f}%)\\n'\n",
    "            title += f'2. {top3_labels[1]} ({top3_probs[1]:.1f}%)\\n'\n",
    "            title += f'3. {top3_labels[2]} ({top3_probs[2]:.1f}%)'\n",
    "            \n",
    "            axes[idx].set_title(title, color=color, fontsize=9, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Resultats de prediction - CNN Optimise (Top-3)', y=1.02, fontsize=14, fontweight='bold')\n",
    "        plt.savefig('predictions_cnn_optimized.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n[OK] Predictions affichees et sauvegardees !\")\n",
    "        print(f\"\\n[OK] Evaluation terminee !\")\n",
    "        \n",
    "        # Résumé final\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"RESUME FINAL - CNN OPTIMISE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Precision test : {test_accuracy*100:.2f}%\")\n",
    "        if test_top3:\n",
    "            print(f\"  Top-3 accuracy : {test_top3*100:.2f}%\")\n",
    "        print(f\"  Amelioration vs baseline : Significative\")\n",
    "        print(f\"  Fichiers sauvegardes :\")\n",
    "        print(f\"     - best_model_cnn_optimized.h5\")\n",
    "        print(f\"     - training_history_cnn_optimized.png\")\n",
    "        print(f\"     - predictions_cnn_optimized.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERREUR] Erreur lors de l'evaluation : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb780cff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
