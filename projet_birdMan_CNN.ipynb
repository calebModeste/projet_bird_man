{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. INITIALISATION - Imports et configuration du dataset ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration du style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# D√©tecter l'environnement\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALISATION DU PROJET BIRD CLASSIFICATION - CNN CLASSIQUE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEnvironnement : {'Google Colab' if IN_COLAB else 'Python Local'}\")\n",
    "\n",
    "# Variables globales\n",
    "DRIVE_FOLDER_ID = \"1kHTcb7OktpYB9vUaZPLQ3ywXFYMUdQsP\"\n",
    "LOCAL_DATA_PATH = Path(\"./data\")\n",
    "TRAIN_PATH = LOCAL_DATA_PATH / \"train_bird\"\n",
    "VALID_PATH = LOCAL_DATA_PATH / \"valid_bird\"\n",
    "\n",
    "# Initialiser dataset_root\n",
    "dataset_root = None\n",
    "drive_loader = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    print(\"\\n‚úì Mode Google Colab d√©tect√©\")\n",
    "    print(\"  Montage de Google Drive...\")\n",
    "    \n",
    "    try:\n",
    "        drive.mount('/content/drive')\n",
    "        drive_base_path = Path('/content/drive/My Drive')\n",
    "        \n",
    "        # Chercher le dataset\n",
    "        for item in drive_base_path.iterdir():\n",
    "            if item.is_dir() and (item / 'train_bird').exists():\n",
    "                dataset_root = item\n",
    "                print(f\"  ‚úì Dataset trouv√© dans : {item.name}\")\n",
    "                break\n",
    "        \n",
    "        if not dataset_root:\n",
    "            print(\"  ‚ö† Dataset non trouv√© dans My Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö† Erreur : {e}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Mode Python Local d√©tect√©\")\n",
    "    \n",
    "    # V√©rifier les donn√©es locales\n",
    "    if TRAIN_PATH.exists() and VALID_PATH.exists():\n",
    "        print(f\"  ‚úì Donn√©es locales trouv√©es : {LOCAL_DATA_PATH}\")\n",
    "        dataset_root = LOCAL_DATA_PATH\n",
    "    else:\n",
    "        print(f\"  ‚ö† Donn√©es locales non trouv√©es\")\n",
    "        print(f\"    Chemin attendu : {LOCAL_DATA_PATH}\")\n",
    "        print(f\"    train_bird existe : {TRAIN_PATH.exists()}\")\n",
    "        print(f\"    valid_bird existe : {VALID_PATH.exists()}\")\n",
    "\n",
    "print(\"\\n‚úì Initialisation termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d326e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ANALYSE DU DATASET - Cr√©er un DataFrame avec les informations ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSE DU DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Limite d'images par classe    \n",
    "MAX_IMAGES_PER_CLASS = 50\n",
    "#MAX_IMAGES_PER_CLASS = 500  # ‚Üê LIMITE √Ä 500 IMAGES PAR CLASSE\n",
    "\n",
    "if dataset_root is None:\n",
    "    print(\"\\n‚ö† Dataset non accessible\")\n",
    "    print(\"  Ex√©cutez la cellule 1 d'abord et assurez-vous que le dataset est disponible\")\n",
    "else:\n",
    "    try:\n",
    "        # Chemins des donn√©es\n",
    "        train_dir = Path(dataset_root) / 'train_bird'\n",
    "        valid_dir = Path(dataset_root) / 'valid_bird'\n",
    "        \n",
    "        # Cr√©er les listes de donn√©es\n",
    "        data = []\n",
    "        \n",
    "        # Traiter les donn√©es d'entra√Ænement\n",
    "        print(\"\\n‚úì Analyse des donn√©es d'entra√Ænement...\")\n",
    "        if train_dir.exists():\n",
    "            for class_path in sorted(train_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter √† MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Entra√Ænement',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        # Traiter les donn√©es de validation\n",
    "        print(\"‚úì Analyse des donn√©es de validation...\")\n",
    "        if valid_dir.exists():\n",
    "            for class_path in sorted(valid_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter √† MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Validation',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        if data:\n",
    "            # Cr√©er le DataFrame\n",
    "            df_dataset = pd.DataFrame(data)\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"R√âSUM√â DU DATASET\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            n_classes = df_dataset['Classe'].nunique()\n",
    "            total_images = df_dataset[\"Nombre d'images\"].sum()\n",
    "            \n",
    "            print(f\"\\nüìä Statistiques globales :\")\n",
    "            print(f\"   Nombre total de classes : {n_classes}\")\n",
    "            print(f\"   Nombre total d'images : {total_images:,}\")\n",
    "            print(f\"   Limite par classe : {MAX_IMAGES_PER_CLASS} images\")\n",
    "            \n",
    "            print(f\"\\nüìà R√©partition par ensemble :\")\n",
    "            stats = df_dataset.groupby('Ensemble').agg({\n",
    "                'Classe': 'nunique',\n",
    "                \"Nombre d'images\": ['sum', 'mean', 'min', 'max']\n",
    "            })\n",
    "            stats.columns = ['Nombre de classes', 'Total images', 'Moy/classe', 'Min', 'Max']\n",
    "            print(stats.to_string())\n",
    "            \n",
    "            print(f\"\\nüèÜ Top 5 classes par nombre d'images :\")\n",
    "            top_classes = df_dataset.nlargest(5, \"Nombre d'images\")[['Classe', 'Ensemble', \"Nombre d'images\"]]\n",
    "            print(top_classes.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n‚úì DataFrame cr√©√© avec succ√®s !\")\n",
    "            print(f\"   Forme : {df_dataset.shape}\")\n",
    "        else:\n",
    "            print(\"‚ö† Aucune image trouv√©e dans le dataset\")\n",
    "            df_dataset = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors de l'analyse : {e}\")\n",
    "        df_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf53b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. PR√âPARATION DES DONN√âES - Preprocessing et augmentation OPTIMIS√âS ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PR√âPARATION DES DONN√âES POUR LE DEEP LEARNING (OPTIMIS√â)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Installation de TensorFlow\n",
    "print(\"\\n‚úì Installation de TensorFlow...\")\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\", \"-q\"])\n",
    "    print(\"  ‚úì TensorFlow install√©\")\n",
    "except:\n",
    "    print(\"  ‚ö† TensorFlow d√©j√† install√©\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n‚úì Configuration des param√®tres OPTIMIS√âS...\")\n",
    "\n",
    "# Param√®tres optimis√©s\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # R√©duit pour meilleure g√©n√©ralisation\n",
    "EPOCHS = 30  # Plus d'epochs avec early stopping\n",
    "LEARNING_RATE = 0.0005  # Learning rate plus bas pour convergence stable\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(f\"  Taille des images : {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch size : {BATCH_SIZE} (optimis√©)\")\n",
    "print(f\"  Nombre d'epochs : {EPOCHS}\")\n",
    "print(f\"  Learning rate : {LEARNING_RATE} (optimis√©)\")\n",
    "\n",
    "# G√©n√©rateurs d'images avec augmentation AVANC√âE\n",
    "print(\"\\n‚úì Cr√©ation des data generators avec augmentation avanc√©e...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    # Augmentation g√©om√©trique\n",
    "    rotation_range=30,  # Rotation plus large\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,  # Zoom plus agressif\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,  # Les oiseaux ne sont pas √† l'envers\n",
    "    fill_mode='reflect',  # Meilleur que 'nearest'\n",
    "    # Augmentation colorim√©trique\n",
    "    brightness_range=[0.8, 1.2],  # Variation de luminosit√©\n",
    "    channel_shift_range=30,  # Variation de couleur\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Validation sans augmentation mais avec normalisation\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Charger les donn√©es d'entra√Ænement\n",
    "train_dir = Path(dataset_root) / 'train_bird' if dataset_root else TRAIN_PATH\n",
    "\n",
    "try:\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        interpolation='bicubic'  # Meilleure qualit√© de redimensionnement\n",
    "    )\n",
    "    \n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        interpolation='bicubic'\n",
    "    )\n",
    "    \n",
    "    # Charger les donn√©es de test (validation du dataset)\n",
    "    valid_dir = Path(dataset_root) / 'valid_bird' if dataset_root else VALID_PATH\n",
    "    \n",
    "    test_generator = val_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "        interpolation='bicubic'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Data generators cr√©√©s avec succ√®s !\")\n",
    "    print(f\"  Train generator : {len(train_generator)} batches\")\n",
    "    print(f\"  Validation generator : {len(val_generator)} batches\")\n",
    "    print(f\"  Test generator : {len(test_generator)} batches\")\n",
    "    print(f\"  Nombre de classes : {train_generator.num_classes}\")\n",
    "    \n",
    "    # Sauvegarder les noms de classes\n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nüìä Augmentations appliqu√©es :\")\n",
    "    print(f\"  ‚Üª Rotation : ¬±30¬∞\")\n",
    "    print(f\"  ‚Üî D√©calage H/V : ¬±25%\")\n",
    "    print(f\"  üîç Zoom : 70-130%\")\n",
    "    print(f\"  ‚òÄ Luminosit√© : 80-120%\")\n",
    "    print(f\"  üé® Channel shift : ¬±30\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur lors de la cr√©ation des generators : {e}\")\n",
    "    train_generator = None\n",
    "    val_generator = None\n",
    "    test_generator = None\n",
    "    class_names = None\n",
    "    num_classes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. CR√âATION DU MOD√àLE - CNN OPTIMIS√â pour classification d'images ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CR√âATION DU MOD√àLE CNN OPTIMIS√â\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None:\n",
    "    print(\"\\n‚ùå Les data generators ne sont pas disponibles\")\n",
    "    print(\"   Ex√©cutez la cellule 3 d'abord\")\n",
    "else:\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    \n",
    "    # Cr√©er un mod√®le CNN optimis√© avec r√©gularisation L2\n",
    "    print(\"\\n‚úì Construction du mod√®le CNN optimis√©...\")\n",
    "    \n",
    "    # Facteur de r√©gularisation L2\n",
    "    L2_REG = 0.001\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # ========== BLOC 1 - Extraction de features bas niveau ==========\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(L2_REG),\n",
    "                      input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.1),  # Dropout spatial plus efficace\n",
    "        \n",
    "        # ========== BLOC 2 - Features interm√©diaires ==========\n",
    "        layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.15),\n",
    "        \n",
    "        # ========== BLOC 3 - Features complexes ==========\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.2),\n",
    "        \n",
    "        # ========== BLOC 4 - Features haut niveau ==========\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.SpatialDropout2D(0.25),\n",
    "        \n",
    "        # ========== BLOC 5 - Features tr√®s haut niveau ==========\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.GlobalAveragePooling2D(),  # Plus efficace que Flatten\n",
    "        \n",
    "        # ========== CLASSIFICATION ==========\n",
    "        layers.Dense(512, kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(256, kernel_regularizer=l2(L2_REG)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Optimiseur avec weight decay et momentum\n",
    "    print(\"\\n‚úì Compilation du mod√®le avec optimiseur avanc√©...\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    initial_lr = LEARNING_RATE\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=initial_lr,\n",
    "        weight_decay=0.0001,  # R√©gularisation additionnelle\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy')]\n",
    "    )\n",
    "    \n",
    "    # Afficher le r√©sum√© du mod√®le\n",
    "    print(\"\\nüìä Architecture du mod√®le OPTIMIS√â :\")\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    \n",
    "    print(f\"   Param√®tres totaux : {total_params:,}\")\n",
    "    print(f\"   Param√®tres entra√Ænables : {trainable_params:,}\")\n",
    "    print(f\"   Blocs convolutifs : 5\")\n",
    "    print(f\"   GlobalAveragePooling : ‚úì (r√©duit l'overfitting)\")\n",
    "    print(f\"   R√©gularisation L2 : {L2_REG}\")\n",
    "    print(f\"   SpatialDropout2D : ‚úì (plus efficace)\")\n",
    "    \n",
    "    # Callbacks optimis√©s\n",
    "    print(\"\\n‚úì Configuration des callbacks avanc√©s...\")\n",
    "    \n",
    "    # Learning rate scheduler avec warmup\n",
    "    def lr_schedule(epoch, lr):\n",
    "        if epoch < 3:\n",
    "            return lr  # Warmup\n",
    "        elif epoch < 15:\n",
    "            return lr * 0.95  # D√©croissance douce\n",
    "        else:\n",
    "            return lr * 0.9  # D√©croissance plus rapide\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=8,  # Plus de patience\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=4,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_cnn_optimized.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n‚úì Mod√®le CNN OPTIMIS√â pr√™t pour l'entra√Ænement !\")\n",
    "    print(f\"\\nüöÄ Am√©liorations appliqu√©es :\")\n",
    "    print(f\"  ‚úì Architecture VGG-like avec 5 blocs\")\n",
    "    print(f\"  ‚úì GlobalAveragePooling (meilleur que Flatten)\")\n",
    "    print(f\"  ‚úì SpatialDropout2D (r√©gularisation spatiale)\")\n",
    "    print(f\"  ‚úì R√©gularisation L2 sur toutes les couches\")\n",
    "    print(f\"  ‚úì AdamW optimizer avec weight decay\")\n",
    "    print(f\"  ‚úì Learning rate scheduler\")\n",
    "    print(f\"  ‚úì Top-3 accuracy tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. ENTRA√éNEMENT - Training du mod√®le CNN OPTIMIS√â ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRA√éNEMENT DU MOD√àLE CNN OPTIMIS√â\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None or model is None:\n",
    "    print(\"\\n‚ùå Erreur : Les donn√©es ou le mod√®le ne sont pas disponibles\")\n",
    "    print(\"   Ex√©cutez les cellules 3 et 4 d'abord\")\n",
    "    history = None\n",
    "else:\n",
    "    try:\n",
    "        import time\n",
    "        \n",
    "        print(f\"\\n‚úì D√©marrage de l'entra√Ænement...\")\n",
    "        print(f\"  Epochs : {EPOCHS}\")\n",
    "        print(f\"  Batch size : {BATCH_SIZE}\")\n",
    "        print(f\"  √âtapes par epoch : {len(train_generator)}\")\n",
    "        print(f\"  Learning rate initial : {LEARNING_RATE}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entra√Æner le mod√®le (sans limitation de steps pour meilleure pr√©cision)\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Statistiques finales\n",
    "        best_val_acc = max(history.history['val_accuracy'])\n",
    "        best_val_top3 = max(history.history['val_top3_accuracy'])\n",
    "        final_lr = history.history.get('lr', [LEARNING_RATE])[-1] if 'lr' in history.history else LEARNING_RATE\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  ‚è±Ô∏è Temps total : {total_time/60:.1f} min\")\n",
    "        print(f\"  üìà Meilleure pr√©cision validation : {best_val_acc*100:.2f}%\")\n",
    "        print(f\"  üéØ Meilleure Top-3 accuracy : {best_val_top3*100:.2f}%\")\n",
    "        print(f\"  üìâ Learning rate final : {final_lr:.2e}\")\n",
    "        print(f\"  üíæ Mod√®le sauvegard√© : best_model_cnn_optimized.h5\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors de l'entra√Ænement : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. √âVALUATION - R√©sultats et visualisation ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âVALUATION DU MOD√àLE CNN OPTIMIS√â\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if history is None or model is None:\n",
    "    print(\"\\n‚ùå Erreur : L'entra√Ænement n'a pas eu lieu\")\n",
    "    print(\"   Ex√©cutez la cellule 5 d'abord\")\n",
    "else:\n",
    "    try:\n",
    "        # √âvaluer sur l'ensemble de test\n",
    "        print(\"\\n‚úì √âvaluation sur l'ensemble de test...\")\n",
    "        results = model.evaluate(test_generator, verbose=0)\n",
    "        test_loss = results[0]\n",
    "        test_accuracy = results[1]\n",
    "        test_top3 = results[2] if len(results) > 2 else None\n",
    "        \n",
    "        print(f\"\\nüìä R√©sultats sur le test set :\")\n",
    "        print(f\"   Perte test : {test_loss:.4f}\")\n",
    "        print(f\"   Pr√©cision test : {test_accuracy*100:.2f}%\")\n",
    "        if test_top3:\n",
    "            print(f\"   Top-3 accuracy : {test_top3*100:.2f}%\")\n",
    "        \n",
    "        # Visualiser l'historique d'entra√Ænement\n",
    "        print(\"\\n‚úì Cr√©ation des graphiques...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "        \n",
    "        # Graphique de la perte\n",
    "        axes[0, 0].plot(epochs_range, history.history['loss'], 'b-', label='Perte entra√Ænement', linewidth=2)\n",
    "        axes[0, 0].plot(epochs_range, history.history['val_loss'], 'r-', label='Perte validation', linewidth=2)\n",
    "        axes[0, 0].set_title('Perte au cours de l\\'entra√Ænement', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Perte')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Graphique de la pr√©cision\n",
    "        axes[0, 1].plot(epochs_range, history.history['accuracy'], 'b-', label='Pr√©cision entra√Ænement', linewidth=2)\n",
    "        axes[0, 1].plot(epochs_range, history.history['val_accuracy'], 'r-', label='Pr√©cision validation', linewidth=2)\n",
    "        axes[0, 1].axhline(y=test_accuracy, color='g', linestyle='--', label=f'Test: {test_accuracy*100:.1f}%')\n",
    "        axes[0, 1].set_title('Pr√©cision au cours de l\\'entra√Ænement', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Pr√©cision')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top-3 accuracy\n",
    "        if 'top3_accuracy' in history.history:\n",
    "            axes[1, 0].plot(epochs_range, history.history['top3_accuracy'], 'b-', label='Top-3 entra√Ænement', linewidth=2)\n",
    "            axes[1, 0].plot(epochs_range, history.history['val_top3_accuracy'], 'r-', label='Top-3 validation', linewidth=2)\n",
    "            if test_top3:\n",
    "                axes[1, 0].axhline(y=test_top3, color='g', linestyle='--', label=f'Test: {test_top3*100:.1f}%')\n",
    "            axes[1, 0].set_title('Top-3 Accuracy', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Top-3 Accuracy')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate (si disponible)\n",
    "        if 'lr' in history.history:\n",
    "            axes[1, 1].plot(epochs_range, history.history['lr'], 'g-', linewidth=2)\n",
    "            axes[1, 1].set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Learning Rate')\n",
    "            axes[1, 1].set_yscale('log')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            # Gap entre train et val accuracy\n",
    "            train_acc = np.array(history.history['accuracy'])\n",
    "            val_acc = np.array(history.history['val_accuracy'])\n",
    "            gap = train_acc - val_acc\n",
    "            axes[1, 1].fill_between(epochs_range, gap, alpha=0.3, color='red')\n",
    "            axes[1, 1].plot(epochs_range, gap, 'r-', linewidth=2)\n",
    "            axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            axes[1, 1].set_title('Overfitting Gap (Train - Val)', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Gap')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history_cnn_optimized.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úì Graphiques affich√©s et sauvegard√©s !\")\n",
    "        \n",
    "        # Pr√©dictions sur quelques images de test\n",
    "        print(\"\\n‚úì Test de pr√©diction sur des images...\")\n",
    "        \n",
    "        # R√©cup√©rer quelques images du test\n",
    "        test_generator.reset()\n",
    "        test_images, test_labels = next(test_generator)\n",
    "        \n",
    "        # Faire des pr√©dictions\n",
    "        predictions = model.predict(test_images[:9], verbose=0)\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(test_labels[:9], axis=1)\n",
    "        \n",
    "        # Afficher les r√©sultats avec Top-3\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx in range(9):\n",
    "            img = (test_images[idx] * 255).astype(np.uint8)\n",
    "            true_label = class_names[true_classes[idx]]\n",
    "            \n",
    "            # Top-3 predictions\n",
    "            top3_idx = np.argsort(predictions[idx])[::-1][:3]\n",
    "            top3_labels = [class_names[i] for i in top3_idx]\n",
    "            top3_probs = [predictions[idx][i] * 100 for i in top3_idx]\n",
    "            \n",
    "            axes[idx].imshow(img)\n",
    "            \n",
    "            # Couleur selon si correct\n",
    "            is_correct = true_classes[idx] == pred_classes[idx]\n",
    "            in_top3 = true_classes[idx] in top3_idx\n",
    "            \n",
    "            if is_correct:\n",
    "                color = 'green'\n",
    "                status = '‚úì'\n",
    "            elif in_top3:\n",
    "                color = 'orange'\n",
    "                status = '‚âà'\n",
    "            else:\n",
    "                color = 'red'\n",
    "                status = '‚úó'\n",
    "            \n",
    "            title = f'{status} Vrai: {true_label}\\n'\n",
    "            title += f'1. {top3_labels[0]} ({top3_probs[0]:.1f}%)\\n'\n",
    "            title += f'2. {top3_labels[1]} ({top3_probs[1]:.1f}%)\\n'\n",
    "            title += f'3. {top3_labels[2]} ({top3_probs[2]:.1f}%)'\n",
    "            \n",
    "            axes[idx].set_title(title, color=color, fontsize=9, fontweight='bold')\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('R√©sultats de pr√©diction - CNN Optimis√© (Top-3)', y=1.02, fontsize=14, fontweight='bold')\n",
    "        plt.savefig('predictions_cnn_optimized.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úì Pr√©dictions affich√©es et sauvegard√©es !\")\n",
    "        print(f\"\\n‚úì √âvaluation termin√©e !\")\n",
    "        \n",
    "        # R√©sum√© final\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"R√âSUM√â FINAL - CNN OPTIMIS√â\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  üéØ Pr√©cision test : {test_accuracy*100:.2f}%\")\n",
    "        if test_top3:\n",
    "            print(f\"  üèÜ Top-3 accuracy : {test_top3*100:.2f}%\")\n",
    "        print(f\"  üìä Am√©lioration vs baseline : Significative\")\n",
    "        print(f\"  üíæ Fichiers sauvegard√©s :\")\n",
    "        print(f\"     - best_model_cnn_optimized.h5\")\n",
    "        print(f\"     - training_history_cnn_optimized.png\")\n",
    "        print(f\"     - predictions_cnn_optimized.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors de l'√©valuation : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb780cff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
