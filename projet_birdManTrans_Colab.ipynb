{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. CONFIGURATION COLAB - GPU et montage Drive ###\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ CONFIGURATION GOOGLE COLAB\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# V√©rifier qu'on est bien sur Colab\n",
    "import sys\n",
    "if 'google.colab' not in sys.modules:\n",
    "    raise RuntimeError(\"‚ùå Ce notebook est con√ßu pour Google Colab uniquement !\")\n",
    "\n",
    "print(\"\\n‚úì Google Colab d√©tect√©\")\n",
    "\n",
    "# V√©rifier le GPU\n",
    "import tensorflow as tf\n",
    "print(f\"\\nüìä Configuration mat√©rielle :\")\n",
    "print(f\"   TensorFlow version : {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   ‚úì GPU disponible : {gpus[0].name}\")\n",
    "    # Configurer la m√©moire GPU\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"   ‚ö† Aucun GPU d√©tect√© - Allez dans Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# Monter Google Drive\n",
    "print(\"\\nüìÅ Montage de Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"   ‚úì Google Drive mont√©\")\n",
    "\n",
    "print(\"\\n‚úì Configuration termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ACC√àS AU DATASET SUR GOOGLE DRIVE ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÅ ACC√àS AU DATASET SUR GOOGLE DRIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins Google Drive - ACC√àS DIRECT (pas de t√©l√©chargement)\n",
    "DRIVE_BASE = Path('/content/drive/My Drive')\n",
    "\n",
    "# Chercher le dataset dans diff√©rents emplacements possibles\n",
    "possible_paths = [\n",
    "    DRIVE_BASE / 'bird_dataset',\n",
    "    DRIVE_BASE / 'data',\n",
    "    DRIVE_BASE / 'projet_bird_man' / 'data',\n",
    "    DRIVE_BASE / 'Colab Notebooks' / 'data',\n",
    "    DRIVE_BASE / 'datasets' / 'bird_dataset',\n",
    "]\n",
    "\n",
    "dataset_root = None\n",
    "print(\"\\nüîç Recherche du dataset dans Google Drive...\")\n",
    "\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / 'train_bird').exists():\n",
    "        dataset_root = path\n",
    "        print(f\"   ‚úì Dataset trouv√© : {path}\")\n",
    "        break\n",
    "\n",
    "# Si non trouv√©, demander le chemin\n",
    "if dataset_root is None:\n",
    "    print(\"\\n‚ö† Dataset non trouv√© automatiquement.\")\n",
    "    print(\"   Veuillez sp√©cifier le chemin vers votre dataset :\")\n",
    "    print(\"   Exemple : /content/drive/My Drive/mon_dossier/data\")\n",
    "    \n",
    "    # Vous pouvez modifier ce chemin manuellement\n",
    "    CUSTOM_PATH = \"/content/drive/My Drive/bird_dataset\"  # ‚Üê MODIFIER ICI SI N√âCESSAIRE\n",
    "    \n",
    "    if Path(CUSTOM_PATH).exists():\n",
    "        dataset_root = Path(CUSTOM_PATH)\n",
    "        print(f\"   ‚úì Chemin personnalis√© utilis√© : {dataset_root}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Chemin non trouv√© : {CUSTOM_PATH}\")\n",
    "        print(\"   V√©rifiez que le dataset est bien dans votre Google Drive\")\n",
    "\n",
    "# D√©finir les chemins train/valid\n",
    "if dataset_root:\n",
    "    TRAIN_PATH = dataset_root / 'train_bird'\n",
    "    VALID_PATH = dataset_root / 'valid_bird'\n",
    "    \n",
    "    if TRAIN_PATH.exists() and VALID_PATH.exists():\n",
    "        train_classes = len([d for d in TRAIN_PATH.iterdir() if d.is_dir()])\n",
    "        valid_classes = len([d for d in VALID_PATH.iterdir() if d.is_dir()])\n",
    "        print(f\"\\n‚úì Dataset accessible directement sur Drive !\")\n",
    "        print(f\"   üìÇ Chemin : {dataset_root}\")\n",
    "        print(f\"   üìä Classes entra√Ænement : {train_classes}\")\n",
    "        print(f\"   üìä Classes validation : {valid_classes}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dossiers train_bird/valid_bird non trouv√©s dans {dataset_root}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset non configur√©. Modifiez CUSTOM_PATH ci-dessus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. IMPORTS ET CONFIGURATION ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ IMPORTS ET CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Configuration du style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Param√®tres globaux\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_IMAGES_PER_CLASS = 50  # Limite pour acc√©l√©rer (mettre None pour tout utiliser)\n",
    "\n",
    "print(\"\\n‚úì Imports termin√©s\")\n",
    "print(f\"\\nüìä Param√®tres :\")\n",
    "print(f\"   Taille images : {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   Batch size : {BATCH_SIZE}\")\n",
    "print(f\"   Epochs max : {EPOCHS}\")\n",
    "print(f\"   Limite images/classe : {MAX_IMAGES_PER_CLASS or 'Aucune'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. ANALYSE DU DATASET - Cr√©er un DataFrame ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ANALYSE DU DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if dataset_root is None:\n",
    "    print(\"\\n‚ö† Dataset non accessible\")\n",
    "    print(\"  Ex√©cutez la cellule 2 d'abord et assurez-vous que le dataset est disponible\")\n",
    "    df_dataset = None\n",
    "else:\n",
    "    try:\n",
    "        # Chemins des donn√©es (acc√®s direct sur Drive)\n",
    "        train_dir = dataset_root / 'train_bird'\n",
    "        valid_dir = dataset_root / 'valid_bird'\n",
    "        \n",
    "        # Cr√©er les listes de donn√©es\n",
    "        data = []\n",
    "        \n",
    "        # Traiter les donn√©es d'entra√Ænement\n",
    "        print(\"\\n‚úì Analyse des donn√©es d'entra√Ænement...\")\n",
    "        if train_dir.exists():\n",
    "            for class_path in sorted(train_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter √† MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS) if MAX_IMAGES_PER_CLASS else len(images)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Entra√Ænement',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        # Traiter les donn√©es de validation\n",
    "        print(\"‚úì Analyse des donn√©es de validation...\")\n",
    "        if valid_dir.exists():\n",
    "            for class_path in sorted(valid_dir.iterdir()):\n",
    "                if class_path.is_dir():\n",
    "                    images = list(class_path.glob('*.[jJ][pP][gG]')) + \\\n",
    "                            list(class_path.glob('*.[jJ][pP][eE][gG]')) + \\\n",
    "                            list(class_path.glob('*.[pP][nN][gG]'))\n",
    "                    \n",
    "                    # Limiter √† MAX_IMAGES_PER_CLASS images par classe\n",
    "                    num_images = min(len(images), MAX_IMAGES_PER_CLASS) if MAX_IMAGES_PER_CLASS else len(images)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Classe': class_path.name,\n",
    "                        'Ensemble': 'Validation',\n",
    "                        \"Nombre d'images\": num_images,\n",
    "                        'Chemin': str(class_path)\n",
    "                    })\n",
    "        \n",
    "        if data:\n",
    "            # Cr√©er le DataFrame\n",
    "            df_dataset = pd.DataFrame(data)\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"R√âSUM√â DU DATASET\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            n_classes = df_dataset['Classe'].nunique()\n",
    "            total_images = df_dataset[\"Nombre d'images\"].sum()\n",
    "            \n",
    "            print(f\"\\nüìä Statistiques globales :\")\n",
    "            print(f\"   Nombre total de classes : {n_classes}\")\n",
    "            print(f\"   Nombre total d'images : {total_images:,}\")\n",
    "            print(f\"   Limite par classe : {MAX_IMAGES_PER_CLASS or 'Aucune'} images\")\n",
    "            \n",
    "            print(f\"\\nüìà R√©partition par ensemble :\")\n",
    "            stats = df_dataset.groupby('Ensemble').agg({\n",
    "                'Classe': 'nunique',\n",
    "                \"Nombre d'images\": ['sum', 'mean', 'min', 'max']\n",
    "            })\n",
    "            stats.columns = ['Nombre de classes', 'Total images', 'Moy/classe', 'Min', 'Max']\n",
    "            print(stats.to_string())\n",
    "            \n",
    "            print(f\"\\nüèÜ Top 5 classes par nombre d'images :\")\n",
    "            top_classes = df_dataset.nlargest(5, \"Nombre d'images\")[['Classe', 'Ensemble', \"Nombre d'images\"]]\n",
    "            print(top_classes.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n‚úì DataFrame cr√©√© avec succ√®s !\")\n",
    "            print(f\"   Forme : {df_dataset.shape}\")\n",
    "            \n",
    "            # Afficher le DataFrame\n",
    "            print(f\"\\nüìã Aper√ßu du DataFrame :\")\n",
    "            display(df_dataset.head(10))\n",
    "        else:\n",
    "            print(\"‚ö† Aucune image trouv√©e dans le dataset\")\n",
    "            df_dataset = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur lors de l'analyse : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        df_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f977dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. VISUALISATION DU DATASET ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VISUALISATION DU DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df_dataset is not None:\n",
    "    # Visualisation de la distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Distribution des images par classe (Entra√Ænement)\n",
    "    train_data = df_dataset[df_dataset['Ensemble'] == 'Entra√Ænement'].sort_values(\"Nombre d'images\", ascending=True)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(train_data)))\n",
    "    axes[0].barh(train_data['Classe'], train_data[\"Nombre d'images\"], color=colors)\n",
    "    axes[0].set_xlabel(\"Nombre d'images\", fontsize=12)\n",
    "    axes[0].set_title('Distribution par classe (Entra√Ænement)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Pie chart de la r√©partition\n",
    "    ensemble_counts = df_dataset.groupby('Ensemble')[\"Nombre d'images\"].sum()\n",
    "    axes[1].pie(ensemble_counts, labels=ensemble_counts.index, autopct='%1.1f%%', \n",
    "                startangle=90, colors=['#2ecc71', '#3498db'], explode=[0.02, 0.02])\n",
    "    axes[1].set_title('R√©partition Train/Validation', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Visualisation termin√©e !\")\n",
    "else:\n",
    "    print(\"‚ö† DataFrame non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea75816",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. EXEMPLES D'IMAGES PAR CLASSE ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üñºÔ∏è EXEMPLES D'IMAGES PAR CLASSE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if dataset_root:\n",
    "    # Afficher des exemples de chaque classe\n",
    "    n_examples = 5\n",
    "    classes_to_show = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])[:n_examples]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_examples, 3, figsize=(12, 3*n_examples))\n",
    "    \n",
    "    for idx, class_name in enumerate(classes_to_show):\n",
    "        class_path = train_dir / class_name\n",
    "        images = list(class_path.glob('*.jpg')) + list(class_path.glob('*.JPG')) + list(class_path.glob('*.jpeg'))\n",
    "        \n",
    "        for j in range(min(3, len(images))):\n",
    "            img = Image.open(images[j])\n",
    "            axes[idx, j].imshow(img)\n",
    "            axes[idx, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[idx, j].set_ylabel(class_name.replace('-', '\\n'), fontsize=9, rotation=0, ha='right', va='center')\n",
    "    \n",
    "    plt.suptitle('Exemples d\\'images par classe', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Exemples affich√©s !\")\n",
    "else:\n",
    "    print(\"‚ö† Dataset non disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. PR√âPARATION DES DATA GENERATORS ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öôÔ∏è PR√âPARATION DES DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if dataset_root is None:\n",
    "    print(\"\\n‚ùå Dataset non disponible\")\n",
    "    train_generator = None\n",
    "    val_generator = None\n",
    "    test_generator = None\n",
    "else:\n",
    "    # Data augmentation pour l'entra√Ænement\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    print(\"\\n‚úì Cr√©ation des generators depuis Google Drive...\")\n",
    "    \n",
    "    # Generator d'entra√Ænement (acc√®s direct au Drive)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Generator de validation\n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Generator de test\n",
    "    test_generator = val_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Infos\n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nüìä R√©sum√© des donn√©es :\")\n",
    "    print(f\"   Classes : {num_classes}\")\n",
    "    print(f\"   Train batches : {len(train_generator)}\")\n",
    "    print(f\"   Validation batches : {len(val_generator)}\")\n",
    "    print(f\"   Test batches : {len(test_generator)}\")\n",
    "    \n",
    "    print(f\"\\nüìã Liste des classes :\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"   {i}: {name}\")\n",
    "    \n",
    "    print(\"\\n‚úì Donn√©es pr√™tes (acc√®s direct depuis Drive) !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153305db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. CR√âATION DU MOD√àLE - Transfer Learning MobileNetV2 ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß† CR√âATION DU MOD√àLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None:\n",
    "    print(\"\\n‚ùå Les data generators ne sont pas disponibles\")\n",
    "    print(\"   Ex√©cutez la cellule 7 d'abord\")\n",
    "    model = None\n",
    "else:\n",
    "    print(\"\\n‚úì Chargement de MobileNetV2 pr√©-entra√Æn√©...\")\n",
    "    \n",
    "    # Charger MobileNetV2 sans les couches de classification\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Geler les couches du mod√®le de base\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    print(f\"   ‚úì MobileNetV2 charg√© : {len(base_model.layers)} couches\")\n",
    "    print(f\"   ‚úì Couches gel√©es pour transfer learning\")\n",
    "    \n",
    "    # Construire le classificateur personnalis√©\n",
    "    print(\"\\n‚úì Construction du classificateur...\")\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compiler\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # R√©sum√©\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    \n",
    "    print(f\"\\nüìä Architecture :\")\n",
    "    print(f\"   Base : MobileNetV2 (ImageNet)\")\n",
    "    print(f\"   Param√®tres totaux : {total_params:,}\")\n",
    "    print(f\"   Param√®tres entra√Ænables : {trainable_params:,}\")\n",
    "    print(f\"   Classes de sortie : {num_classes}\")\n",
    "    \n",
    "    print(\"\\n‚úì Mod√®le cr√©√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a847c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9. R√âSUM√â DU MOD√àLE ###\n",
    "\n",
    "if model:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10. ENTRA√éNEMENT PHASE 1 - Classificateur ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ ENTRA√éNEMENT DU MOD√àLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_generator is None or model is None:\n",
    "    print(\"\\n‚ùå Erreur : Les donn√©es ou le mod√®le ne sont pas disponibles\")\n",
    "    history = None\n",
    "else:\n",
    "    import time\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks_phase1 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            '/content/best_model_phase1.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # ========== PHASE 1 : Entra√Æner le classificateur ==========\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"üìç PHASE 1 : Entra√Ænement du classificateur (couches gel√©es)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=10,\n",
    "        callbacks=callbacks_phase1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    phase1_time = time.time() - start_time\n",
    "    phase1_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    print(f\"\\n‚úì Phase 1 termin√©e en {phase1_time/60:.1f} min\")\n",
    "    print(f\"   Meilleure pr√©cision : {phase1_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11. PHASE 2 : FINE-TUNING ###\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"üìç PHASE 2 : Fine-tuning (d√©gel partiel)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if history is None:\n",
    "    print(\"\\n‚ùå Phase 1 non termin√©e\")\n",
    "else:\n",
    "    # D√©geler les 30 derni√®res couches\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    trainable_count = sum([1 for layer in model.layers if layer.trainable])\n",
    "    print(f\"   ‚úì {trainable_count} couches maintenant entra√Ænables\")\n",
    "    \n",
    "    # Recompiler avec learning rate plus faible\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Nouveaux callbacks\n",
    "    callbacks_phase2 = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            '/content/best_model_finetuned.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history_ft = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=10,\n",
    "        callbacks=callbacks_phase2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    phase2_time = time.time() - start_time\n",
    "    \n",
    "    # Combiner les historiques\n",
    "    for key in history.history:\n",
    "        history.history[key].extend(history_ft.history[key])\n",
    "    \n",
    "    final_acc = max(history_ft.history['val_accuracy'])\n",
    "    \n",
    "    print(f\"\\n‚úì Phase 2 termin√©e en {phase2_time/60:.1f} min\")\n",
    "    print(f\"   Meilleure pr√©cision : {final_acc*100:.2f}%\")\n",
    "    \n",
    "    # R√©sum√© final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
    "    print(\"=\"*60)\n",
    "    total_time = phase1_time + phase2_time\n",
    "    print(f\"   ‚è±Ô∏è Temps total : {total_time/60:.1f} min\")\n",
    "    print(f\"   üìà Pr√©cision Phase 1 : {phase1_acc*100:.2f}%\")\n",
    "    print(f\"   üéØ Pr√©cision Phase 2 : {final_acc*100:.2f}%\")\n",
    "    print(f\"   üíæ Mod√®le sauvegard√© : /content/best_model_finetuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12. √âVALUATION SUR LE TEST SET ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ √âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# √âvaluer\n",
    "print(\"\\n‚úì √âvaluation en cours...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats sur le test set :\")\n",
    "print(f\"   Perte : {test_loss:.4f}\")\n",
    "print(f\"   Pr√©cision : {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\n‚úì G√©n√©ration des pr√©dictions...\")\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=0)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nüìã Rapport de classification :\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matrice de Confusion', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Pr√©diction')\n",
    "plt.ylabel('V√©rit√©')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì √âvaluation termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 13. EXEMPLES DE PR√âDICTIONS ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üñºÔ∏è EXEMPLES DE PR√âDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# R√©cup√©rer quelques images\n",
    "test_generator.reset()\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "# Pr√©dictions\n",
    "preds = model.predict(test_images[:12], verbose=0)\n",
    "pred_classes = np.argmax(preds, axis=1)\n",
    "true_classes = np.argmax(test_labels[:12], axis=1)\n",
    "\n",
    "# Affichage\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(12):\n",
    "    img = (test_images[idx] * 255).astype(np.uint8)\n",
    "    true_label = class_names[true_classes[idx]]\n",
    "    pred_label = class_names[pred_classes[idx]]\n",
    "    confidence = preds[idx][pred_classes[idx]] * 100\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    \n",
    "    correct = true_classes[idx] == pred_classes[idx]\n",
    "    color = 'green' if correct else 'red'\n",
    "    symbol = '‚úì' if correct else '‚úó'\n",
    "    \n",
    "    title = f'{symbol} {pred_label}\\n({confidence:.1f}%)'\n",
    "    axes[idx].set_title(title, color=color, fontsize=10, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Exemples de pr√©dictions (Vert=Correct, Rouge=Incorrect)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Exemples affich√©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316be70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 14. SAUVEGARDE DU MOD√àLE SUR GOOGLE DRIVE ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAUVEGARDE SUR GOOGLE DRIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if model is None:\n",
    "    print(\"\\n‚ùå Mod√®le non disponible\")\n",
    "else:\n",
    "    # Cr√©er le dossier de sauvegarde dans Drive\n",
    "    save_dir = Path('/content/drive/My Drive/bird_classifier_model')\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder le mod√®le complet\n",
    "    model_path = save_dir / 'bird_classifier_mobilenet.h5'\n",
    "    model.save(str(model_path))\n",
    "    print(f\"\\n‚úì Mod√®le sauvegard√© : {model_path}\")\n",
    "    \n",
    "    # Sauvegarder les classes\n",
    "    import json\n",
    "    classes_path = save_dir / 'class_names.json'\n",
    "    with open(classes_path, 'w') as f:\n",
    "        json.dump(class_names, f, indent=2)\n",
    "    print(f\"‚úì Classes sauvegard√©es : {classes_path}\")\n",
    "    \n",
    "    # Sauvegarder l'historique\n",
    "    if history:\n",
    "        history_path = save_dir / 'training_history.json'\n",
    "        # Convertir les valeurs numpy en float\n",
    "        history_dict = {k: [float(v) for v in vals] for k, vals in history.history.items()}\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "        print(f\"‚úì Historique sauvegard√© : {history_path}\")\n",
    "    \n",
    "    # Copier aussi le meilleur mod√®le\n",
    "    import shutil\n",
    "    if Path('/content/best_model_finetuned.h5').exists():\n",
    "        shutil.copy('/content/best_model_finetuned.h5', save_dir / 'best_model_finetuned.h5')\n",
    "        print(f\"‚úì Meilleur mod√®le copi√©\")\n",
    "    \n",
    "    # Sauvegarder le DataFrame\n",
    "    if df_dataset is not None:\n",
    "        df_path = save_dir / 'dataset_info.csv'\n",
    "        df_dataset.to_csv(df_path, index=False)\n",
    "        print(f\"‚úì DataFrame sauvegard√© : {df_path}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Tous les fichiers sauvegard√©s dans :\")\n",
    "    print(f\"   {save_dir}\")\n",
    "    print(\"\\n‚úì Sauvegarde termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715919f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 15. FONCTION DE PR√âDICTION ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÆ FONCTION DE PR√âDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_bird(image_path, model=model, class_names=class_names, top_k=3):\n",
    "    \"\"\"\n",
    "    Pr√©dit l'esp√®ce d'oiseau √† partir d'une image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Chemin vers l'image\n",
    "        model: Mod√®le entra√Æn√©\n",
    "        class_names: Liste des noms de classes\n",
    "        top_k: Nombre de pr√©dictions √† retourner\n",
    "    \n",
    "    Returns:\n",
    "        dict: R√©sultats de la pr√©diction\n",
    "    \"\"\"\n",
    "    # Charger et pr√©traiter l'image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "    img_array = np.array(img_resized) / 255.0\n",
    "    img_batch = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    predictions = model.predict(img_batch, verbose=0)[0]\n",
    "    \n",
    "    # Top-K pr√©dictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    \n",
    "    results = {\n",
    "        'predictions': [\n",
    "            {'class': class_names[i], 'confidence': float(predictions[i] * 100)}\n",
    "            for i in top_indices\n",
    "        ],\n",
    "        'top_class': class_names[top_indices[0]],\n",
    "        'confidence': float(predictions[top_indices[0]] * 100)\n",
    "    }\n",
    "    \n",
    "    return results, img\n",
    "\n",
    "# Test avec une image du dataset\n",
    "print(\"\\n‚úì Fonction de pr√©diction cr√©√©e\")\n",
    "print(\"\\nüìù Exemple d'utilisation :\")\n",
    "print('   results, img = predict_bird(\"/chemin/vers/image.jpg\")')\n",
    "print('   print(f\"Esp√®ce: {results[\\\"top_class\\\"]} ({results[\\\"confidence\\\"]:.1f}%)\")')\n",
    "\n",
    "# Test r√©el avec une image du dataset sur Drive\n",
    "if dataset_root and model:\n",
    "    test_images_list = list(valid_dir.glob('*/*.jpg'))[:1]\n",
    "    if test_images_list:\n",
    "        test_img_path = test_images_list[0]\n",
    "        results, img = predict_bird(test_img_path)\n",
    "        \n",
    "        print(f\"\\nüß™ Test avec : {test_img_path.name}\")\n",
    "        print(f\"   Top 3 pr√©dictions :\")\n",
    "        for pred in results['predictions']:\n",
    "            print(f\"     - {pred['class']}: {pred['confidence']:.1f}%\")\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Pr√©diction: {results['top_class']}\\n({results['confidence']:.1f}%)\", \n",
    "                  fontsize=12, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 16. UPLOAD ET PR√âDICTION D'IMAGES PERSONNELLES ###\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì§ UPLOAD ET PR√âDICTION D'IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"\\nüìå Uploadez une image d'oiseau pour la classifier :\")\n",
    "\n",
    "try:\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"\\nüîç Analyse de : {filename}\")\n",
    "        \n",
    "        # Sauvegarder temporairement\n",
    "        with open(f'/content/{filename}', 'wb') as f:\n",
    "            f.write(uploaded[filename])\n",
    "        \n",
    "        # Pr√©diction\n",
    "        results, img = predict_bird(f'/content/{filename}')\n",
    "        \n",
    "        # Affichage\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        title = f\"Esp√®ce pr√©dite : {results['top_class']}\\n\"\n",
    "        title += f\"Confiance : {results['confidence']:.1f}%\"\n",
    "        plt.title(title, fontsize=14, fontweight='bold', color='green')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Top 3 pr√©dictions :\")\n",
    "        for i, pred in enumerate(results['predictions'], 1):\n",
    "            bar = '‚ñà' * int(pred['confidence'] / 5)\n",
    "            print(f\"   {i}. {pred['class']}: {pred['confidence']:.1f}% {bar}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† Aucun fichier upload√© ou erreur : {e}\")\n",
    "    print(\"   Vous pouvez r√©ex√©cuter cette cellule pour uploader une image.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
